x     = "Trigger (0→5 left→right)",
y     = "Query   (0→5 top→bottom)"
) +
theme(
axis.text.x = element_text(angle = 45, hjust = 1),
panel.grid  = element_blank(),
strip.text  = element_text(size = 10)
)
print(p)
# 7) Save to file
# ggsave(
#   filename = OUTPUT_PNG,
#   plot     = p,
#   width    = 12,
#   height   = 8,
#   dpi      = 300
# )
# after you’ve built p
ggsave(
filename = "~/laila_johnston@brown.edu - Google Drive/Shared drives/BLT Lab/Current Studies/SCA (Structure of Computational Alternatives; formerly CompAlt and RSA)/Extensions (Laila)/figures/empirical_probability_set.png",
plot     = p,
width    = 12,
height   = 8,
dpi      = 300,
bg = "white"
)
df <- read_csv(ordering_results, col_types = cols(
context        = col_character(),
trigger        = col_character(),
query          = col_character(),
empirical_probability    = col_double()
)) %>%
rename(
cleaned_trigger = trigger,
cleaned_query   = query
)
# 2) Compute the average empirical probability per (context, trigger, query)
avg_df <- df %>%
group_by(context, cleaned_trigger, cleaned_query) %>%
summarise(
mean_empirical = mean(empirical_probability, na.rm = TRUE),
.groups = "drop"
)
# 4) Merge in the relevance scores
plot_df <- avg_df %>%
# bring in trigger_relevance
left_join(
sca %>% select(story, cleaned_trigger, trigger_relevance) %>% distinct(),
by = c("context" = "story", "cleaned_trigger")
) %>%
# bring in query_relevance
left_join(
sca %>% select(story, cleaned_query, query_relevance) %>% distinct(),
by = c("context" = "story", "cleaned_query")
)
plot_df <- plot_df %>%
mutate(
trigger_ord = reorder_within(cleaned_trigger,
trigger_relevance,
context,
fun = mean),
query_ord   = reorder_within(cleaned_query,
-query_relevance,
context,
fun = mean)
)
# 6) Plot with facet_wrap and the green→red fill
p <- ggplot(plot_df,
aes(x = trigger_ord,
y = query_ord,
fill = mean_empirical)) +
geom_tile(color = "white") +
# add text labels, rounded to two decimals
geom_text(aes(label = sprintf("%.2f", mean_empirical)),
size = 3, color = "black") +
scale_fill_gradient(low = "green", high = "red",
name = "Avg Empirical P") +
facet_wrap(~ context, scales = "free") +
scale_x_reordered() +
scale_y_reordered() +
theme_minimal(base_size = 12) +
labs(
title = "Average Empirical Probability by Context\nOrdering Model\nPrompt: 'I only have [MASK]'",
x     = "Trigger (0→5 left→right)",
y     = "Query   (0→5 top→bottom)"
) +
theme(
axis.text.x = element_text(angle = 45, hjust = 1),
panel.grid  = element_blank(),
strip.text  = element_text(size = 10)
)
print(p)
# 7) Save to file
# ggsave(
#   filename = OUTPUT_PNG,
#   plot     = p,
#   width    = 12,
#   height   = 8,
#   dpi      = 300
# )
# after you’ve built p
ggsave(
filename = "~/laila_johnston@brown.edu - Google Drive/Shared drives/BLT Lab/Current Studies/SCA (Structure of Computational Alternatives; formerly CompAlt and RSA)/Extensions (Laila)/figures/empirical_probability_ordering.png",
plot     = p,
width    = 12,
height   = 8,
dpi      = 300,
bg = "white"
)
df <- read_csv(always_negate_results, col_types = cols(
context        = col_character(),
trigger        = col_character(),
query          = col_character(),
empirical_probability    = col_double()
)) %>%
rename(
cleaned_trigger = trigger,
cleaned_query   = query
)
# 2) Compute the average empirical probability per (context, trigger, query)
avg_df <- df %>%
group_by(context, cleaned_trigger, cleaned_query) %>%
summarise(
mean_empirical = mean(empirical_probability, na.rm = TRUE),
.groups = "drop"
)
# 4) Merge in the relevance scores
plot_df <- avg_df %>%
# bring in trigger_relevance
left_join(
sca %>% select(story, cleaned_trigger, trigger_relevance) %>% distinct(),
by = c("context" = "story", "cleaned_trigger")
) %>%
# bring in query_relevance
left_join(
sca %>% select(story, cleaned_query, query_relevance) %>% distinct(),
by = c("context" = "story", "cleaned_query")
)
plot_df <- plot_df %>%
mutate(
trigger_ord = reorder_within(cleaned_trigger,
trigger_relevance,
context,
fun = mean),
query_ord   = reorder_within(cleaned_query,
-query_relevance,
context,
fun = mean)
)
# 6) Plot with facet_wrap and the green→red fill
p <- ggplot(plot_df,
aes(x = trigger_ord,
y = query_ord,
fill = mean_empirical)) +
geom_tile(color = "white") +
# add text labels, rounded to two decimals
geom_text(aes(label = sprintf("%.2f", mean_empirical)),
size = 3, color = "black") +
scale_fill_gradient(low = "green", high = "red",
name = "Avg Empirical P") +
facet_wrap(~ context, scales = "free") +
scale_x_reordered() +
scale_y_reordered() +
theme_minimal(base_size = 12) +
labs(
title = "Average Empirical Probability by Context\Always Negate\nPrompt: 'I only have [MASK]'",
# 6) Plot with facet_wrap and the green→red fill
p <- ggplot(plot_df,
aes(x = trigger_ord,
y = query_ord,
fill = mean_empirical)) +
geom_tile(color = "white") +
# add text labels, rounded to two decimals
geom_text(aes(label = sprintf("%.2f", mean_empirical)),
size = 3, color = "black") +
scale_fill_gradient(low = "green", high = "red",
name = "Avg Empirical P") +
facet_wrap(~ context, scales = "free") +
scale_x_reordered() +
scale_y_reordered() +
theme_minimal(base_size = 12) +
labs(
title = "Average Empirical Probability by Context\nAlways Negate\nPrompt: 'I only have [MASK]'",
x     = "Trigger (0→5 left→right)",
y     = "Query   (0→5 top→bottom)"
) +
theme(
axis.text.x = element_text(angle = 45, hjust = 1),
panel.grid  = element_blank(),
strip.text  = element_text(size = 10)
)
print(p)
# 7) Save to file
# ggsave(
#   filename = OUTPUT_PNG,
#   plot     = p,
#   width    = 12,
#   height   = 8,
#   dpi      = 300
# )
# after you’ve built p
ggsave(
filename = "~/laila_johnston@brown.edu - Google Drive/Shared drives/BLT Lab/Current Studies/SCA (Structure of Computational Alternatives; formerly CompAlt and RSA)/Extensions (Laila)/figures/empirical_probability_always_negate.png",
plot     = p,
width    = 12,
height   = 8,
dpi      = 300,
bg = "white"
)
df <- read_csv(never_negate_results, col_types = cols(
context        = col_character(),
trigger        = col_character(),
query          = col_character(),
empirical_probability    = col_double()
)) %>%
rename(
cleaned_trigger = trigger,
cleaned_query   = query
)
# 2) Compute the average empirical probability per (context, trigger, query)
avg_df <- df %>%
group_by(context, cleaned_trigger, cleaned_query) %>%
summarise(
mean_empirical = mean(empirical_probability, na.rm = TRUE),
.groups = "drop"
)
# 4) Merge in the relevance scores
plot_df <- avg_df %>%
# bring in trigger_relevance
left_join(
sca %>% select(story, cleaned_trigger, trigger_relevance) %>% distinct(),
by = c("context" = "story", "cleaned_trigger")
) %>%
# bring in query_relevance
left_join(
sca %>% select(story, cleaned_query, query_relevance) %>% distinct(),
by = c("context" = "story", "cleaned_query")
)
plot_df <- plot_df %>%
mutate(
trigger_ord = reorder_within(cleaned_trigger,
trigger_relevance,
context,
fun = mean),
query_ord   = reorder_within(cleaned_query,
-query_relevance,
context,
fun = mean)
)
# 6) Plot with facet_wrap and the green→red fill
p <- ggplot(plot_df,
aes(x = trigger_ord,
y = query_ord,
fill = mean_empirical)) +
geom_tile(color = "white") +
# add text labels, rounded to two decimals
geom_text(aes(label = sprintf("%.2f", mean_empirical)),
size = 3, color = "black") +
scale_fill_gradient(low = "green", high = "red",
name = "Avg Empirical P") +
facet_wrap(~ context, scales = "free") +
scale_x_reordered() +
scale_y_reordered() +
theme_minimal(base_size = 12) +
labs(
title = "Average Empirical Probability by Context\Never Negate\nPrompt: 'I only have [MASK]'",
p <- ggplot(plot_df,
aes(x = trigger_ord,
y = query_ord,
fill = mean_empirical)) +
geom_tile(color = "white") +
# add text labels, rounded to two decimals
geom_text(aes(label = sprintf("%.2f", mean_empirical)),
size = 3, color = "black") +
scale_fill_gradient(low = "green", high = "red",
name = "Avg Empirical P") +
facet_wrap(~ context, scales = "free") +
scale_x_reordered() +
scale_y_reordered() +
theme_minimal(base_size = 12) +
labs(
title = "Average Empirical Probability by Context\nNever Negate\nPrompt: 'I only have [MASK]'",
x     = "Trigger (0→5 left→right)",
y     = "Query   (0→5 top→bottom)"
) +
theme(
axis.text.x = element_text(angle = 45, hjust = 1),
panel.grid  = element_blank(),
strip.text  = element_text(size = 10)
)
print(p)
# 7) Save to file
# ggsave(
#   filename = OUTPUT_PNG,
#   plot     = p,
#   width    = 12,
#   height   = 8,
#   dpi      = 300
# )
# after you’ve built p
ggsave(
filename = "~/laila_johnston@brown.edu - Google Drive/Shared drives/BLT Lab/Current Studies/SCA (Structure of Computational Alternatives; formerly CompAlt and RSA)/Extensions (Laila)/figures/empirical_probability_never_negate.png",
plot     = p,
width    = 12,
height   = 8,
dpi      = 300,
bg = "white"
)
df <- read_csv(conjunction_results, col_types = cols(
context        = col_character(),
trigger        = col_character(),
query          = col_character(),
empirical_probability    = col_double()
)) %>%
rename(
cleaned_trigger = trigger,
cleaned_query   = query
)
# 2) Compute the average empirical probability per (context, trigger, query)
avg_df <- df %>%
group_by(context, cleaned_trigger, cleaned_query) %>%
summarise(
mean_empirical = mean(empirical_probability, na.rm = TRUE),
.groups = "drop"
)
# 4) Merge in the relevance scores
plot_df <- avg_df %>%
# bring in trigger_relevance
left_join(
sca %>% select(story, cleaned_trigger, trigger_relevance) %>% distinct(),
by = c("context" = "story", "cleaned_trigger")
) %>%
# bring in query_relevance
left_join(
sca %>% select(story, cleaned_query, query_relevance) %>% distinct(),
by = c("context" = "story", "cleaned_query")
)
plot_df <- plot_df %>%
mutate(
trigger_ord = reorder_within(cleaned_trigger,
trigger_relevance,
context,
fun = mean),
query_ord   = reorder_within(cleaned_query,
-query_relevance,
context,
fun = mean)
)
# 6) Plot with facet_wrap and the green→red fill
p <- ggplot(plot_df,
aes(x = trigger_ord,
y = query_ord,
fill = mean_empirical)) +
geom_tile(color = "white") +
# add text labels, rounded to two decimals
geom_text(aes(label = sprintf("%.2f", mean_empirical)),
size = 3, color = "black") +
scale_fill_gradient(low = "green", high = "red",
name = "Avg Empirical P") +
facet_wrap(~ context, scales = "free") +
scale_x_reordered() +
scale_y_reordered() +
theme_minimal(base_size = 12) +
labs(
title = "Average Empirical Probability by Context\nConjunction Model\nPrompt: 'I only have [MASK]'",
x     = "Trigger (0→5 left→right)",
y     = "Query   (0→5 top→bottom)"
) +
theme(
axis.text.x = element_text(angle = 45, hjust = 1),
panel.grid  = element_blank(),
strip.text  = element_text(size = 10)
)
print(p)
# 7) Save to file
# ggsave(
#   filename = OUTPUT_PNG,
#   plot     = p,
#   width    = 12,
#   height   = 8,
#   dpi      = 300
# )
# after you’ve built p
ggsave(
filename = "~/laila_johnston@brown.edu - Google Drive/Shared drives/BLT Lab/Current Studies/SCA (Structure of Computational Alternatives; formerly CompAlt and RSA)/Extensions (Laila)/figures/empirical_probability_conjunction.png",
plot     = p,
width    = 12,
height   = 8,
dpi      = 300,
bg = "white"
)
df <- read_csv(disjunction_results, col_types = cols(
context        = col_character(),
trigger        = col_character(),
query          = col_character(),
empirical_probability    = col_double()
)) %>%
rename(
cleaned_trigger = trigger,
cleaned_query   = query
)
# 2) Compute the average empirical probability per (context, trigger, query)
avg_df <- df %>%
group_by(context, cleaned_trigger, cleaned_query) %>%
summarise(
mean_empirical = mean(empirical_probability, na.rm = TRUE),
.groups = "drop"
)
# 4) Merge in the relevance scores
plot_df <- avg_df %>%
# bring in trigger_relevance
left_join(
sca %>% select(story, cleaned_trigger, trigger_relevance) %>% distinct(),
by = c("context" = "story", "cleaned_trigger")
) %>%
# bring in query_relevance
left_join(
sca %>% select(story, cleaned_query, query_relevance) %>% distinct(),
by = c("context" = "story", "cleaned_query")
)
plot_df <- plot_df %>%
mutate(
trigger_ord = reorder_within(cleaned_trigger,
trigger_relevance,
context,
fun = mean),
query_ord   = reorder_within(cleaned_query,
-query_relevance,
context,
fun = mean)
)
# 6) Plot with facet_wrap and the green→red fill
p <- ggplot(plot_df,
aes(x = trigger_ord,
y = query_ord,
fill = mean_empirical)) +
geom_tile(color = "white") +
# add text labels, rounded to two decimals
geom_text(aes(label = sprintf("%.2f", mean_empirical)),
size = 3, color = "black") +
scale_fill_gradient(low = "green", high = "red",
name = "Avg Empirical P") +
facet_wrap(~ context, scales = "free") +
scale_x_reordered() +
scale_y_reordered() +
theme_minimal(base_size = 12) +
labs(
title = "Average Empirical Probability by Context\nDisjunction Model\nPrompt: 'I only have [MASK]'",
x     = "Trigger (0→5 left→right)",
y     = "Query   (0→5 top→bottom)"
) +
theme(
axis.text.x = element_text(angle = 45, hjust = 1),
panel.grid  = element_blank(),
strip.text  = element_text(size = 10)
)
print(p)
# 7) Save to file
# ggsave(
#   filename = OUTPUT_PNG,
#   plot     = p,
#   width    = 12,
#   height   = 8,
#   dpi      = 300
# )
# after you’ve built p
ggsave(
filename = "~/laila_johnston@brown.edu - Google Drive/Shared drives/BLT Lab/Current Studies/SCA (Structure of Computational Alternatives; formerly CompAlt and RSA)/Extensions (Laila)/figures/empirical_probability_disjunction.png",
plot     = p,
width    = 12,
height   = 8,
dpi      = 300,
bg = "white"
)
View(df)
# Add a model column and combine the dataframes
set_results$model <- "Set"
ordering_results$model <- "Ordering"
conjunction_results$model <- "Conjunction"
disjunction_results$model <- "Disjunction"
always_negate_results$model = "Always Negate"
never_negate_results$model = "Never Negate"
df_results <- bind_rows(set_results, ordering_results, conjunction_results, disjunction_results, always_negate_results, never_negate_results)
# Import result files
set_results = read_csv("~/laila_johnston@brown.edu - Google Drive/Shared drives/BLT Lab/Current Studies/SCA (Structure of Computational Alternatives; formerly CompAlt and RSA)/Extensions (Laila)/results/set_results.csv")
ordering_results <- read_csv("~/laila_johnston@brown.edu - Google Drive/Shared drives/BLT Lab/Current Studies/SCA (Structure of Computational Alternatives; formerly CompAlt and RSA)/Extensions (Laila)/results/ordering_results.csv")
conjunction_results <- read_csv("~/laila_johnston@brown.edu - Google Drive/Shared drives/BLT Lab/Current Studies/SCA (Structure of Computational Alternatives; formerly CompAlt and RSA)/Extensions (Laila)/results/conjunction_results.csv")
disjunction_results <- read_csv("~/laila_johnston@brown.edu - Google Drive/Shared drives/BLT Lab/Current Studies/SCA (Structure of Computational Alternatives; formerly CompAlt and RSA)/Extensions (Laila)/results/disjunction_results.csv")
always_negate_results = read_csv("~/laila_johnston@brown.edu - Google Drive/Shared drives/BLT Lab/Current Studies/SCA (Structure of Computational Alternatives; formerly CompAlt and RSA)/Extensions (Laila)/results/always_negate_results.csv")
never_negate_results = read_csv("~/laila_johnston@brown.edu - Google Drive/Shared drives/BLT Lab/Current Studies/SCA (Structure of Computational Alternatives; formerly CompAlt and RSA)/Extensions (Laila)/results/never_negate_results.csv")
# Add a model column and combine the dataframes
set_results$model <- "Set"
ordering_results$model <- "Ordering"
conjunction_results$model <- "Conjunction"
disjunction_results$model <- "Disjunction"
always_negate_results$model = "Always Negate"
never_negate_results$model = "Never Negate"
df_results <- bind_rows(set_results, ordering_results, conjunction_results, disjunction_results, always_negate_results, never_negate_results)
# Compute the mean of the log_likelihoods and the standard error of the log_likelihoods for each set_boundary and model
summary_df <- df_results %>%
group_by(set_boundary, model) %>%
summarise(
mean_log_likelihood = mean(log_likelihood, na.rm = TRUE),
se_log_likelihood = sd(log_likelihood, na.rm = TRUE) / sqrt(n()))
# Plot of the mean log_likelihoods as a function of set boundary (four lines, one for each model) with standard error of the mean
ggplot(summary_df, aes(x = set_boundary, y = mean_log_likelihood, color = model)) +
geom_line() +
geom_point() +
geom_errorbar(aes(ymin = mean_log_likelihood - se_log_likelihood, ymax = mean_log_likelihood + se_log_likelihood), width = 0.2) +
labs(x = "Set Boundary", y = "Log Likelihood", title = "Mean of log likelihoods  as a function of set boundary with standard error of the mean\nAveraged accross all contexts and trials") +
theme_minimal() +
theme(legend.title = element_blank())
